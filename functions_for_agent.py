import os
import json
import requests
import numpy as np
import concurrent.futures
from pathlib import Path
from typing import List

import fitz  # PyMuPDF
from sklearn.metrics.pairwise import cosine_distances
from langchain.schema import Document

# LangChain
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.base import Embeddings
from langchain_gigachat import GigaChat
from langchain.chains import RetrievalQA
from langchain.schema import AIMessage
from langchain.prompts import PromptTemplate

import os
import fitz
from transformers import AutoTokenizer
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_gigachat import GigaChat

import tempfile
import shutil
from langchain.schema import Document

from config import BOT_TOKEN, AUTH

from phoenix.otel import register
from openinference.instrumentation.langchain import LangChainInstrumentor
import phoenix as px

tracer_provider = register()
LangChainInstrumentor().instrument(tracer_provider=tracer_provider)
px.launch_app()

def get_token():
    url = "https://ngw.devices.sberbank.ru:9443/api/v2/oauth"
    headers = {
        'Content-Type': 'application/x-www-form-urlencoded',
        'Accept': 'application/json',
        'RqUID': '450ce936-9e4b-4e1c-aa70-a197e3890e8f',
        'Authorization': f'Basic {AUTH}'
    }
    payload = "scope=GIGACHAT_API_CORP"
    response = requests.post(url, headers=headers, data=payload, verify=False)
    return response.json()['access_token']

session = requests.Session()
global_token = get_token()

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("ai-forever/sbert_large_nlu_ru")

def truncate_to_token_limit(text: str, max_tokens: int = 500) -> str:
    tokens = tokenizer.encode(text, add_special_tokens=False)
    return tokenizer.decode(tokens[:max_tokens], skip_special_tokens=True)

def embeddings(texts, model="Embeddings"):
    url = "https://gigachat.devices.sberbank.ru/api/v1/embeddings"
    headers = {
        'Content-Type': 'application/json',
        'Accept': 'application/json',
        'Authorization': f'Bearer {global_token}'
    }

    results = []
    batch_size = 5

    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        safe_batch = []

        for t in batch:
            if isinstance(t, str) and t.strip():
                trimmed = truncate_to_token_limit(t)
                safe_batch.append(trimmed)

        if not safe_batch:
            continue

        payload = json.dumps({"model": model, "input": safe_batch})
        response = session.post(url, headers=headers, data=payload, verify=False)

        if response.status_code != 200:
            raise Exception(f"–û—à–∏–±–∫–∞ –≤ embeddings: {response.status_code}: {response.text}")

        results.extend(response.json()["data"])

    return results



def compare_news(s1, s2):
    embs = embeddings([s1, s2])
    vect1 = np.array(embs[0]["embedding"]).reshape(1, -1)
    vect2 = np.array(embs[1]["embedding"]).reshape(1, -1)
    return cosine_distances(vect1, vect2)[0][0]


def process_candidate(candidate, query):
    try:
        dist = compare_news(query, candidate)
        return candidate, dist
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ —Å '{candidate}': {e}")
        return candidate, float("inf")


def get_best_match_parallel(query, candidates, max_workers=30):
    results = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(process_candidate, c, query): c for c in candidates}
        for future in concurrent.futures.as_completed(futures):
            candidate, dist = future.result()
            results.append((candidate, dist))
    results.sort(key=lambda x: x[1])
    return results[0][0]


def extract_title_from_pdf(path):
    doc = fitz.open(path)
    lines = doc[0].get_text().split('\n')
    return lines[0].strip() if lines else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"

def find_best_contract_template(user_query: str, data_root="data") -> str:

    import difflib

    print(f"üìÅ –ò—â—É –ª—É—á—à—É—é –ø–∞–ø–∫—É —Å—Ä–µ–¥–∏: {os.listdir(data_root)}")
    folders = [f for f in os.listdir(data_root) if os.path.isdir(os.path.join(data_root, f))]
    if not folders:
        raise ValueError("‚ùó –ù–µ—Ç –ø–∞–ø–æ–∫ —Å —à–∞–±–ª–æ–Ω–∞–º–∏ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏.")

    # 1. –ü–æ–∏—Å–∫ –ø–æ–¥—Ö–æ–¥—è—â–µ–π –ø–∞–ø–∫–∏
    folder_scores = []
    for folder in folders:
        try:
            dist = compare_news(user_query, folder)
            folder_scores.append((folder, dist))
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Å –ø–∞–ø–∫–æ–π '{folder}': {e}")
            continue

    folder_scores.sort(key=lambda x: x[1])
    best_folder = folder_scores[0][0]
    folder_path = os.path.join(data_root, best_folder)
    print(f"üìÅ –õ—É—á—à–∞—è –ø–∞–ø–∫–∞: {folder_path}")

    # 2. –ü–æ–∏—Å–∫ —Ç–æ–ø-5 —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ
    files = [f for f in os.listdir(folder_path) if f.endswith(".pdf")]
    if not files:
        raise ValueError("‚ùó –í –≤—ã–±—Ä–∞–Ω–Ω–æ–π –ø–∞–ø–∫–µ –Ω–µ—Ç PDF-—Ñ–∞–π–ª–æ–≤.")

    file_scores = []
    for f in files:
        try:
            dist = compare_news(user_query, f)
            file_scores.append((f, dist))
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Å —Ñ–∞–π–ª–æ–º '{f}': {e}")
            continue

    file_scores.sort(key=lambda x: x[1])
    top_files = file_scores[:5]
    options_block = "\n".join([f"{i+1}. {f[0]}" for i, f in enumerate(top_files)])

    print("üìÑ –¢–æ–ø-5 —à–∞–±–ª–æ–Ω–æ–≤:")
    print(options_block)

    # 3. –í–æ–ø—Ä–æ—Å –≤ GigaChat
    prompt = PromptTemplate(
    input_variables=["contract_text", "options"],
    template="""
–¢—ã –ø–æ–º–æ—â–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–æ–≥–∞–µ—Ç –≤—ã–±—Ä–∞—Ç—å –ª—É—á—à–∏–π —à–∞–±–ª–æ–Ω –¥–æ–≥–æ–≤–æ—Ä–∞. –í–æ—Ç –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:

"{contract_text}"

–í–æ—Ç —Å–ø–∏—Å–æ–∫ 5 –Ω–∞–∏–±–æ–ª–µ–µ –±–ª–∏–∑–∫–∏—Ö —à–∞–±–ª–æ–Ω–æ–≤:

{options}

–ï—Å–ª–∏ –≤ —Å–ø–∏—Å–∫–µ –Ω–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ —à–∞–±–ª–æ–Ω–∞, –ø—Ä–æ—Å—Ç–æ –æ—Ç–≤–µ—Ç—å:
"–ü–æ–∫–∞ —É –º–µ–Ω—è –Ω–µ—Ç —Ç–∞–∫–æ–≥–æ —à–∞–±–ª–æ–Ω–∞. –ü–æ–ø—Ä–æ–±—É–π –ø–æ–∑–∂–µ."

–í –ª—é–±–æ–º —Å–ª—É—á–∞–µ, –æ—Ç–≤–µ—Ç—å —Ç–æ—á–Ω—ã–º –Ω–∞–∑–≤–∞–Ω–∏–µ–º —Ñ–∞–π–ª–∞ –∏–∑ —Å–ø–∏—Å–∫–∞ –∏–ª–∏ —ç—Ç–æ–π —Ñ—Ä–∞–∑–æ–π.
"""
)


    llm = GigaChat(
        credentials=AUTH,
        verify_ssl_certs=False,
        scope='GIGACHAT_API_CORP',
        model="GigaChat-2-Max",
        profanity_check=False
    )

    chain = LLMChain(llm=llm, prompt=prompt)
    result = chain.invoke({
        "contract_text": user_query,
        "options": options_block
    })

    result_text = result["text"].strip()

    # –ï—Å–ª–∏ GigaChat —á–µ—Å—Ç–Ω–æ –æ—Ç–≤–µ—Ç–∏–ª, —á—Ç–æ –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ñ–ª–∞–≥ –∏ —Å–æ–æ–±—â–µ–Ω–∏–µ
    if result_text.startswith("–ü–æ–∫–∞ —É –º–µ–Ω—è –Ω–µ—Ç —Ç–∞–∫–æ–≥–æ —à–∞–±–ª–æ–Ω–∞"):
        print("ü§ñ –û—Ç–≤–µ—Ç –æ—Ç GigaChat: —à–∞–±–ª–æ–Ω–∞ –Ω–µ—Ç.")
        return {
            "status": "not_found",
            "message": "–ü–æ–∫–∞ —É –º–µ–Ω—è –Ω–µ—Ç —Ç–∞–∫–æ–≥–æ —à–∞–±–ª–æ–Ω–∞. –ü–æ–ø—Ä–æ–±—É–π –ø–æ–∑–∂–µ."
        }

    # –ò–Ω–∞—á–µ ‚Äî –∏—â–µ–º –±–ª–∏–∂–∞–π—à–µ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
    all_filenames = [f[0] for f in top_files]
    closest_match = difflib.get_close_matches(result_text, all_filenames, n=1, cutoff=0.5)

    if closest_match:
        selected_file = closest_match[0]
        final_path = os.path.join(folder_path, selected_file)
        print(f"üìå GigaChat –≤—ã–±—Ä–∞–ª: {final_path}")
        return {
            "status": "ok",
            "path": final_path
        }
    else:
        raise FileNotFoundError(f"‚ùó GigaChat –≤—ã–±—Ä–∞–ª —Ñ–∞–π–ª '{result_text}', –Ω–æ –æ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω —Å—Ä–µ–¥–∏ —Ç–æ–ø-5.")

class GigaChatEmbeddings(Embeddings):
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        safe_texts = []
        for t in texts:
            if isinstance(t, Document):
                t = t.page_content
            if not isinstance(t, str):
                continue
            t = t.strip()
            if not t:
                continue
            safe_texts.append(t[:1500])  # —É—Å–µ—á—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        return [item["embedding"] for item in embeddings(safe_texts)]

    def embed_query(self, text: str) -> List[float]:
        return self.embed_documents([text])[0]

def answer_question_about_contract(pdf_path: str, question: str) -> str:
    from langchain.prompts import PromptTemplate

    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ—á–∏—Å—Ç–∫–∞ PDF
    loader = PyMuPDFLoader(pdf_path)
    pages = loader.load()

    for doc in pages:
        doc.page_content = doc.page_content.replace("\n", " ")

    # –î–µ–ª–∏–º –Ω–∞ —á–∞–Ω–∫–∏
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=700,
        chunk_overlap=100
    )
    chunks = splitter.split_documents(pages)

    # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
    embedding_model = GigaChatEmbeddings()
    with tempfile.TemporaryDirectory() as chroma_dir:
        vector_db = Chroma.from_documents(chunks, embedding_model, persist_directory=chroma_dir)
        retriever = vector_db.as_retriever(search_type="mmr", search_kwargs={"k": 10})  # üëà –∏—â–µ–º –±–æ–ª—å—à–µ —á–∞–Ω–∫–æ–≤
        relevant_docs = retriever.get_relevant_documents(question)

    # –ö–æ—Å–∏–Ω—É—Å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è (–æ—Å–ª–∞–±–ª–µ–Ω–∞)
    question_emb = np.array(embedding_model.embed_query(question)).reshape(1, -1)
    accepted_docs = []
    print("\nüîç –ö–æ—Å–∏–Ω—É—Å–Ω—ã–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞:", question)
    for i, doc in enumerate(relevant_docs):
        if not isinstance(doc, Document):
            continue
        doc_text = doc.page_content.strip()
        if not doc_text:
            continue
        doc_emb = np.array(embedding_model.embed_query(doc_text)).reshape(1, -1)
        distance = cosine_distances(question_emb, doc_emb)[0][0]
        print(f"[{i}] –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {distance:.4f} | –¢–µ–∫—Å—Ç: {doc_text[:120].replace(chr(10), ' ')}...")
        if distance < 0.35:  # üëà –±–æ–ª–µ–µ –º—è–≥–∫–∏–π —Ñ–∏–ª—å—Ç—Ä
            accepted_docs.append(Document(page_content=doc_text, metadata=doc.metadata or {}))

    if not accepted_docs:
        print("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —á–∞–Ω–∫–æ–≤ –ø–æ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –º–µ—Ç—Ä–∏–∫–µ ‚Äî –ø–µ—Ä–µ–¥–∞—ë–º –≤—Å—ë –≤ LLM")
        accepted_docs = relevant_docs  # üëà –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—ë, —á—Ç–æ –≤–µ—Ä–Ω—É–ª retriever

    # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    with tempfile.TemporaryDirectory() as filtered_dir:
        filtered_db = Chroma.from_documents(accepted_docs, embedding_model, persist_directory=filtered_dir)
        retriever_filtered = filtered_db.as_retriever(search_type="mmr", search_kwargs={"k": 5})

        llm = GigaChat(
            credentials=AUTH,
            verify_ssl_certs=False,
            scope='GIGACHAT_API_CORP',
            model="GigaChat-2-Max",
            profanity_check=False
        )

        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            retriever=retriever_filtered,
            chain_type="stuff",
            return_source_documents=True,
            verbose=False,
            chain_type_kwargs={
                "prompt": PromptTemplate(
                    input_variables=["context", "question"],
                    template="""
–¢—ã –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –¥–æ–≥–æ–≤–æ—Ä—É. –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
–ï—Å–ª–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–µ—Ç ‚Äî –ø—Ä–æ—Å—Ç–æ —Å–∫–∞–∂–∏: "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–æ–≥–æ–≤–æ—Ä–µ."

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–í–æ–ø—Ä–æ—Å: {question}
–û—Ç–≤–µ—Ç:"""
                )
            }
        )

        response = qa_chain.invoke({"query": question})
        return response["result"]


def legal_audit_from_gk(pdf_path: str, kodex_dir: str = "kodex") -> str:
    from langchain_community.document_loaders import PyMuPDFLoader
    from langchain.chains import LLMChain
    import tempfile

    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –¥–æ–≥–æ–≤–æ—Ä–∞
    loader = PyMuPDFLoader(pdf_path)
    contract_pages = loader.load()
    for doc in contract_pages:
        doc.page_content = doc.page_content.replace("\n", " ")
    contract_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=150)
    contract_chunks = contract_splitter.split_documents(contract_pages)

    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ì–ö
    all_kodex_docs = []
    kodex_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
    for filename in sorted(os.listdir(kodex_dir)):
        if filename.endswith(".pdf"):
            path = os.path.join(kodex_dir, filename)
            kodex_loader = PyMuPDFLoader(path)
            kodex_pages = kodex_loader.load()
            for doc in kodex_pages:
                doc.page_content = doc.page_content.replace("\n", " ")
            all_kodex_docs.extend(kodex_splitter.split_documents(kodex_pages))

    print(f"üìë –î–æ–≥–æ–≤–æ—Ä: {len(contract_chunks)} —á–∞–Ω–∫–æ–≤ | –ì–ö: {len(all_kodex_docs)} —Å—Ç–∞—Ç–µ–π")

    # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
    embedding_model = GigaChatEmbeddings()

    with tempfile.TemporaryDirectory() as temp_dir:
        kodex_db = Chroma.from_documents(
            all_kodex_docs, embedding_model, persist_directory=os.path.join(temp_dir, "kodex")
        )
        retriever = kodex_db.as_retriever(search_type="mmr", search_kwargs={"k": 8})

        # –ú–æ–¥–µ–ª—å
        llm = GigaChat(
            credentials=AUTH,
            verify_ssl_certs=False,
            scope='GIGACHAT_API_CORP',
            model="GigaChat-2-Max",
            profanity_check=False
        )

        # –ü—Ä–æ–º–ø—Ç + —Ü–µ–ø–æ—á–∫–∞
        prompt = PromptTemplate(
            input_variables=["context", "contract"],
            template="""
–¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π —é—Ä–∏—Å—Ç. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –¥–∞–Ω–Ω—ã–π –¥–æ–≥–æ–≤–æ—Ä –Ω–∞—Ä—É—à–µ–Ω–∏—è –ø–æ–ª–æ–∂–µ–Ω–∏–π –ì—Ä–∞–∂–¥–∞–Ω—Å–∫–æ–≥–æ –∫–æ–¥–µ–∫—Å–∞ –†–§.

–ö–æ–Ω—Ç–µ–∫—Å—Ç (—Å—Ç–∞—Ç—å–∏ –ì–ö –†–§):
{context}

–§—Ä–∞–≥–º–µ–Ω—Ç—ã –¥–æ–≥–æ–≤–æ—Ä–∞:
{contract}

–í—ã–≤–µ–¥–∏ –≤–æ–∑–º–æ–∂–Ω—ã–µ –Ω–∞—Ä—É—à–µ–Ω–∏—è –∏ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç–∞—Ç—å–∏. –ï—Å–ª–∏ –≤—Å—ë –≤ –ø–æ—Ä—è–¥–∫–µ, –Ω–∞–ø–∏—à–∏: "–ù–∞—Ä—É—à–µ–Ω–∏–π –Ω–µ –≤—ã—è–≤–ª–µ–Ω–æ."
"""
        )
        qa_chain = LLMChain(llm=llm, prompt=prompt)

        # –ê–Ω–∞–ª–∏–∑ –ø–æ —á–∞–Ω–∫–∞–º
        violations = []
        for i in range(0, len(contract_chunks), 3):
            partial_contract_text = truncate_to_token_limit(
                " ".join(c.page_content for c in contract_chunks[i:i + 3]), 450
            )
            relevant_articles = retriever.get_relevant_documents(partial_contract_text)
            context_text = truncate_to_token_limit(
                "\n\n".join(d.page_content for d in relevant_articles), 800
            )

            result = qa_chain.invoke({
                "context": context_text,
                "contract": partial_contract_text
            })

            violations.append(f"üîπ –§—Ä–∞–≥–º–µ–Ω—Ç {i + 1}‚Äì{i + 3}:\n{result}\n")

        # –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç
        final_report = "\n".join(violations)
        report_path = "gk_audit_report.txt"
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(final_report)

        return report_path  # –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É ‚Äî –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏
    
def detect_contract_topic_gigachat(pdf_path: str, topics: List[str]) -> str:
    from langchain.prompts import PromptTemplate
    from langchain.chains import LLMChain

    # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –¥–æ–≥–æ–≤–æ—Ä–∞ (—É—Å–µ–∫–∞–µ–º –ø–æ —Ç–æ–∫–µ–Ω–∞–º)
    doc = fitz.open(pdf_path)
    text = "\n".join(page.get_text() for page in doc)
    trimmed_text = truncate_to_token_limit(text, max_tokens=900)

    # 2. –ì–æ—Ç–æ–≤–∏–º —Å–ø–∏—Å–æ–∫ —Ç–µ–º –∫–∞–∫ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É
    options_block = "\n".join([f"- {t}" for t in topics])

    # 3. –ü—Ä–æ–º–ø—Ç
    prompt = PromptTemplate(
        input_variables=["contract_text", "options"],
        template="""
–¢—ã ‚Äî —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å, –∫ –∫–∞–∫–æ–º—É –∏–∑ –Ω–∏–∂–µ–ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–æ–≥–æ–≤–æ—Ä–æ–≤ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –ø—Ä–∏–≤–µ–¥—ë–Ω–Ω—ã–π –Ω–∏–∂–µ —Ç–µ–∫—Å—Ç.

–í—ã–±–µ—Ä–∏ **—Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–π** –≤–∞—Ä–∏–∞–Ω—Ç –∏–∑ —Å–ø–∏—Å–∫–∞. –ï—Å–ª–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –Ω–µ—Ç, –Ω–∞–ø–∏—à–∏: "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å".

–¢–∏–ø—ã –¥–æ–≥–æ–≤–æ—Ä–æ–≤:
{options}

–¢–µ–∫—Å—Ç –¥–æ–≥–æ–≤–æ—Ä–∞:
{contract_text}

–û—Ç–≤–µ—Ç:
"""
    )

    # 4. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º GigaChat
    llm = GigaChat(
        credentials=AUTH,
        verify_ssl_certs=False,
        scope='GIGACHAT_API_CORP',
        model="GigaChat-2-Max",
        profanity_check=False
    )

    # 5. –¶–µ–ø–æ—á–∫–∞ –∏ –∑–∞–ø—É—Å–∫
    chain = LLMChain(llm=llm, prompt=prompt)
    result = chain.invoke({
        "contract_text": trimmed_text,
        "options": options_block
    })
    
    print(result)
    return result["text"].strip() 



# –û–¥–∏–Ω —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞ –≤—Å–µ
tokenizer = AutoTokenizer.from_pretrained("ai-forever/sbert_large_nlu_ru")

def truncate(text: str, max_tokens: int = 800) -> str:
    tokens = tokenizer.encode(text, add_special_tokens=False)
    return tokenizer.decode(tokens[:max_tokens], skip_special_tokens=True)

# –ú–∞–ø–∞: —Ç–µ–º–∞ ‚Üí —Ñ–∞–π–ª
TOPIC_TO_KODEX_PDF = {
    "–û–±—Ä–∞–∑—Ü—ã –¥–æ–≥–æ–≤–æ—Ä–æ–≤ –∞—Ä–µ–Ω–¥—ã": "–°—Ç–∞—Ç—å–∏_–¥–æ–≥–æ–≤–æ—Ä–∞_–∞—Ä–µ–Ω–¥—ã_–∫–≤–∞—Ä—Ç–∏—Ä—ã,_–Ω–µ–∂–∏–ª–æ–≥–æ_–ø–æ–º–µ—â–µ–Ω–∏—è,_–≥–∞—Ä–∞–∂–∞,_–∑–µ–º–ª–∏.pdf",
    "–û–±—Ä–∞–∑—Ü—ã –¥–æ–≥–æ–≤–æ—Ä–æ–≤ –∞—Ä–µ–Ω–¥—ã –∫–≤–∞—Ä—Ç–∏—Ä—ã": "–°—Ç–∞—Ç—å–∏_–¥–æ–≥–æ–≤–æ—Ä–∞_–∞—Ä–µ–Ω–¥—ã_–∫–≤–∞—Ä—Ç–∏—Ä—ã,_–Ω–µ–∂–∏–ª–æ–≥–æ_–ø–æ–º–µ—â–µ–Ω–∏—è,_–≥–∞—Ä–∞–∂–∞,_–∑–µ–º–ª–∏.pdf",
    "–î–æ–≥–æ–≤–æ—Ä—ã –∞—Ä–µ–Ω–¥—ã –∫–æ–º–Ω–∞—Ç—ã": "–°—Ç–∞—Ç—å–∏ –¥–æ–≥–æ–≤–æ—Ä–∞ –∞—Ä–µ–Ω–¥—ã –∫–≤–∞—Ä—Ç–∏—Ä—ã, –Ω–µ–∂–∏–ª–æ–≥–æ –ø–æ–º–µ—â–µ–Ω–∏—è, –≥–∞—Ä–∞–∂–∞, –∑–µ–º–ª–∏.pdf",
    "–î–æ–≥–æ–≤–æ—Ä—ã –∞—Ä–µ–Ω–¥—ã –≥–∞—Ä–∞–∂–∞": "–°—Ç–∞—Ç—å–∏ –¥–æ–≥–æ–≤–æ—Ä–∞ –∞—Ä–µ–Ω–¥—ã –∫–≤–∞—Ä—Ç–∏—Ä—ã, –Ω–µ–∂–∏–ª–æ–≥–æ –ø–æ–º–µ—â–µ–Ω–∏—è, –≥–∞—Ä–∞–∂–∞, –∑–µ–º–ª–∏.pdf",
    "–î–æ–≥–æ–≤–æ—Ä—ã –∞—Ä–µ–Ω–¥—ã –Ω–µ–∂–∏–ª–æ–≥–æ –ø–æ–º–µ—â–µ–Ω–∏—è": "–°—Ç–∞—Ç—å–∏ –¥–æ–≥–æ–≤–æ—Ä–∞ –∞—Ä–µ–Ω–¥—ã –∫–≤–∞—Ä—Ç–∏—Ä—ã, –Ω–µ–∂–∏–ª–æ–≥–æ –ø–æ–º–µ—â–µ–Ω–∏—è, –≥–∞—Ä–∞–∂–∞, –∑–µ–º–ª–∏.pdf",
    "–î–æ–≥–æ–≤–æ—Ä—ã –∞—Ä–µ–Ω–¥—ã —Ç–æ—Ä–≥–æ–≤–æ–≥–æ –ø–æ–º–µ—â–µ–Ω–∏—è": "–°—Ç–∞—Ç—å–∏ –¥–æ–≥–æ–≤–æ—Ä–∞ –∞—Ä–µ–Ω–¥—ã –∫–≤–∞—Ä—Ç–∏—Ä—ã, –Ω–µ–∂–∏–ª–æ–≥–æ –ø–æ–º–µ—â–µ–Ω–∏—è, –≥–∞—Ä–∞–∂–∞, –∑–µ–º–ª–∏.pdf",
    "–î–æ–≥–æ–≤–æ—Ä—ã –∞—Ä–µ–Ω–¥—ã –∑–¥–∞–Ω–∏–π": "–°—Ç–∞—Ç—å–∏ –¥–æ–≥–æ–≤–æ—Ä–∞ –∞—Ä–µ–Ω–¥—ã –∫–≤–∞—Ä—Ç–∏—Ä—ã, –Ω–µ–∂–∏–ª–æ–≥–æ –ø–æ–º–µ—â–µ–Ω–∏—è, –≥–∞—Ä–∞–∂–∞, –∑–µ–º–ª–∏.pdf",
    "–î–æ–≥–æ–≤–æ—Ä—ã –∞—Ä–µ–Ω–¥—ã –∑–µ–º–µ–ª—å–Ω–æ–≥–æ —É—á–∞—Å—Ç–∫–∞": "–°—Ç–∞—Ç—å–∏ –¥–æ–≥–æ–≤–æ—Ä–∞ –∞—Ä–µ–Ω–¥—ã –∫–≤–∞—Ä—Ç–∏—Ä—ã, –Ω–µ–∂–∏–ª–æ–≥–æ –ø–æ–º–µ—â–µ–Ω–∏—è, –≥–∞—Ä–∞–∂–∞, –∑–µ–º–ª–∏.pdf",
    "–î–æ–≥–æ–≤–æ—Ä—ã –∞—Ä–µ–Ω–¥—ã –º–∞—à–∏–Ω–æ–º–µ—Å—Ç–∞": "–°—Ç–∞—Ç—å–∏ –¥–æ–≥–æ–≤–æ—Ä–∞ –∞—Ä–µ–Ω–¥—ã –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è, —Å–ø–µ—Ü—Ç–µ—Ö–Ω–∏–∫–∏, –º–∞—à–∏–Ω–æ–º–µ—Å—Ç–∞.pdf",
    "–î–æ–≥–æ–≤–æ—Ä—ã –∞—Ä–µ–Ω–¥—ã –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏": "–°—Ç–∞—Ç—å–∏ –¥–æ–≥–æ–≤–æ—Ä–∞ –∞—Ä–µ–Ω–¥—ã –∫–≤–∞—Ä—Ç–∏—Ä—ã, –Ω–µ–∂–∏–ª–æ–≥–æ –ø–æ–º–µ—â–µ–Ω–∏—è, –≥–∞—Ä–∞–∂–∞, –∑–µ–º–ª–∏.pdf",
    "–î–æ–≥–æ–≤–æ—Ä—ã –∞—Ä–µ–Ω–¥—ã —Å–ø–µ—Ü—Ç–µ—Ö–Ω–∏–∫–∏": "–°—Ç–∞—Ç—å–∏ –¥–æ–≥–æ–≤–æ—Ä–∞ –∞—Ä–µ–Ω–¥—ã –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è, —Å–ø–µ—Ü—Ç–µ—Ö–Ω–∏–∫–∏, –º–∞—à–∏–Ω–æ–º–µ—Å—Ç–∞.pdf",
    "–î–æ–≥–æ–≤–æ—Ä—ã –∞—Ä–µ–Ω–¥—ã –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è": "–°—Ç–∞—Ç—å–∏ –¥–æ–≥–æ–≤–æ—Ä–∞ –∞—Ä–µ–Ω–¥—ã –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è, —Å–ø–µ—Ü—Ç–µ—Ö–Ω–∏–∫–∏, –º–∞—à–∏–Ω–æ–º–µ—Å—Ç–∞.pdf",
    "–î–æ–≥–æ–≤–æ—Ä—ã –∞—Ä–µ–Ω–¥—ã –∞–≤—Ç–æ–º–æ–±–∏–ª—è": "–°—Ç–∞—Ç—å–∏ –¥–æ–≥–æ–≤–æ—Ä–∞ –∞—Ä–µ–Ω–¥—ã –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è, —Å–ø–µ—Ü—Ç–µ—Ö–Ω–∏–∫–∏, –º–∞—à–∏–Ω–æ–º–µ—Å—Ç–∞.pdf",
    "–î–æ–≥–æ–≤–æ—Ä—ã –∞—Ä–µ–Ω–¥—ã —Å –ø—Ä–∞–≤–æ–º –≤—ã–∫—É–ø–∞": "–°—Ç–∞—Ç—å–∏ –¥–æ–≥–æ–≤–æ—Ä–∞ –∞—Ä–µ–Ω–¥—ã —Å –ø—Ä–∞–≤–æ–º –≤—ã–∫—É–ø–∞.pdf",
    "–û–±—Ä–∞–∑—Ü—ã –¥–æ–≥–æ–≤–æ—Ä–æ–≤ —Å—É–±–∞—Ä–µ–Ω–¥—ã": "–°—Ç–∞—Ç—å–∏_–¥–æ–≥–æ–≤–æ—Ä–∞_–∞—Ä–µ–Ω–¥—ã_–∫–≤–∞—Ä—Ç–∏—Ä—ã,_–Ω–µ–∂–∏–ª–æ–≥–æ_–ø–æ–º–µ—â–µ–Ω–∏—è,_–≥–∞—Ä–∞–∂–∞,_–∑–µ–º–ª–∏.pdf",
    "–û–±—Ä–∞–∑—Ü—ã –¥–æ–≥–æ–≤–æ—Ä–æ–≤ –∫—É–ø–ª–∏-–ø—Ä–æ–¥–∞–∂–∏": "–°—Ç–∞—Ç—å–∏ –¥–æ–≥–æ–≤–æ—Ä–∞ –∫—É–ø–ª–∏-–ø—Ä–æ–¥–∞–∂–∏.pdf",
    "–î–æ–≥–æ–≤–æ—Ä—ã –∫—É–ø–ª–∏-–ø—Ä–æ–¥–∞–∂–∏ –∞–≤—Ç–æ–º–æ–±–∏–ª—è": "–°—Ç–∞—Ç—å–∏ –¥–æ–≥–æ–≤–æ—Ä–∞ –∫—É–ø–ª–∏-–ø—Ä–æ–¥–∞–∂–∏.pdf",
    "–û–±—Ä–∞–∑—Ü—ã –¥–æ–≥–æ–≤–æ—Ä–æ–≤ –ø–æ–¥—Ä—è–¥–∞": "–°—Ç–∞—Ç—å–∏ –¥–æ–≥–æ–≤–æ—Ä–∞ –ì–ø—Ö.pdf",
    "–û–±—Ä–∞–∑—Ü—ã –¥–æ–≥–æ–≤–æ—Ä–æ–≤ –æ–∫–∞–∑–∞–Ω–∏—è —É—Å–ª—É–≥": "–°—Ç–∞—Ç—å–∏ –¥–æ–≥–æ–≤–æ—Ä–∞ –ì–ø—Ö.pdf",
    "–û–±—Ä–∞–∑—Ü—ã –¥–æ–≥–æ–≤–æ—Ä–æ–≤ –ì–ü–•": "–°—Ç–∞—Ç—å–∏ –¥–æ–≥–æ–≤–æ—Ä–∞ –ì–ø—Ö.pdf",
    "–ê–≥–µ–Ω—Ç—Å–∫–∏–µ –¥–æ–≥–æ–≤–æ—Ä—ã: –æ–±—Ä–∞–∑—Ü—ã": "–°—Ç–∞—Ç—å–∏ –¥–æ–≥–æ–≤–æ—Ä–∞ –ì–ø—Ö.pdf",
    "–®–∞–±–ª–æ–Ω—ã –¥–æ–≥–æ–≤–æ—Ä–æ–≤ –ø–æ—Å—Ç–∞–≤–∫–∏": "–°—Ç–∞—Ç—å–∏ –¥–æ–≥–æ–≤–æ—Ä–∞ –ì–ø—Ö.pdf",
    "–®–∞–±–ª–æ–Ω—ã –¥–æ–≥–æ–≤–æ—Ä–æ–≤ —Ü–µ—Å—Å–∏–∏": "–°—Ç–∞—Ç—å–∏ –¥–æ–≥–æ–≤–æ—Ä–∞ –ì–ø—Ö.pdf",
    "–î–æ–≥–æ–≤–æ—Ä—ã —Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–∞": "–°—Ç–∞—Ç—å–∏ –¥–æ–≥–æ–≤–æ—Ä–∞ –ì–ø—Ö.pdf",
    "–®–∞–±–ª–æ–Ω—ã –¥–æ–≥–æ–≤–æ—Ä–æ–≤ –∞–≤—Ç–æ—Ä—Å–∫–æ–≥–æ –∑–∞–∫–∞–∑–∞": "–°—Ç–∞—Ç—å–∏ –¥–æ–≥–æ–≤–æ—Ä–∞ –ì–ø—Ö.pdf",
    "–û–±—Ä–∞–∑—Ü—ã –¥–æ–≥–æ–≤–æ—Ä–æ–≤ –∑–∞–π–º–∞": "–°—Ç–∞—Ç—å–∏ –∑–∞–π–º-—Ä–∞—Å–ø–∏—Å–∫–∞.pdf",
    "–†–∞—Å–ø–∏—Å–∫–∏ 2025": "–°—Ç–∞—Ç—å–∏ –∑–∞–π–º-—Ä–∞—Å–ø–∏—Å–∫–∞.pdf",
    "–û–±—Ä–∞–∑—Ü—ã –¥–æ–≥–æ–≤–æ—Ä–æ–≤ –ø–æ—Ä—É—á–∏—Ç–µ–ª—å—Å—Ç–≤–∞": "–°—Ç–∞—Ç—å–∏ –¥–æ–≥–æ–≤–æ—Ä–∞ –ø–æ—Ä—É—á–∏—Ç–µ–ª—å—Å—Ç–≤–∞.pdf",
    "–û–±—Ä–∞–∑—Ü—ã –¥–æ–≥–æ–≤–æ—Ä–æ–≤ —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏—è": "–°—Ç–∞—Ç—å–∏ –¥–æ–≥–æ–≤–æ—Ä–∞ –ì–ø—Ö.pdf",
    "–û–±—Ä–∞–∑—Ü—ã –¥–æ–≥–æ–≤–æ—Ä–æ–≤ –¥–∞—Ä–µ–Ω–∏—è": "–°—Ç–∞—Ç—å–∏ –¥–∞—Ä–µ–Ω–∏—è.pdf",
    "–û–±—Ä–∞–∑—Ü—ã –¥–æ–≥–æ–≤–æ—Ä–æ–≤ —Å—Å—É–¥—ã": "–°—Ç–∞—Ç—å–∏ –¥–æ–≥–æ–≤–æ—Ä–∞ —Å—Å—É–¥–∞.pdf",
    "–û–±—Ä–∞–∑—Ü—ã –¥–æ–≥–æ–≤–æ—Ä–æ–≤ —Ö—Ä–∞–Ω–µ–Ω–∏—è": "–°—Ç–∞—Ç—å–∏ –¥–æ–≥–æ–≤–æ—Ä–∞ —Ö—Ä–∞–Ω–µ–Ω–∏—è.pdf",
    "–û–±—Ä–∞–∑—Ü—ã –¥–æ–≥–æ–≤–æ—Ä–æ–≤ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏–º—É—â–µ—Å—Ç–≤–æ–º": "–°—Ç–∞—Ç—å–∏ –¥–æ–≥–æ–≤–æ—Ä–∞ –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ.pdf",
    "–ö–æ–º–∏—Å—Å–∏–æ–Ω–Ω—ã–µ –¥–æ–≥–æ–≤–æ—Ä—ã: –æ–±—Ä–∞–∑—Ü—ã –∏ —à–∞–±–ª–æ–Ω—ã": "–°—Ç–∞—Ç—å–∏ –¥–æ–≥–æ–≤–æ—Ä–∞ –ì–ø—Ö.pdf",
    "–î–æ–≥–æ–≤–æ—Ä—ã –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–π –∫–æ–Ω—Ü–µ—Å—Å–∏–∏": "–°—Ç–∞—Ç—å–∏ –¥–æ–≥–æ–≤–æ—Ä–∞ –ì–ø—Ö.pdf",
    "–û–±—Ä–∞–∑—Ü—ã –¥–æ–≥–æ–≤–æ—Ä–æ–≤ –ª–∏–∑–∏–Ω–≥–∞": "–°—Ç–∞—Ç—å–∏ –¥–æ–≥–æ–≤–æ—Ä–∞ –ì–ø—Ö.pdf",
    "–û–±—Ä–∞–∑—Ü—ã –ª–∏—Ü–µ–Ω–∑–∏–æ–Ω–Ω—ã—Ö –¥–æ–≥–æ–≤–æ—Ä–æ–≤": "–°—Ç–∞—Ç—å–∏ –¥–æ–≥–æ–≤–æ—Ä–∞ –ª–∏—Ü–µ–Ω–∑–∏–∏.pdf",
    "–ë—Ä–∞—á–Ω—ã–µ –¥–æ–≥–æ–≤–æ—Ä—ã: –æ–±—Ä–∞–∑—Ü—ã –∏ –ø—Ä–∏–º–µ—Ä—ã": "–°—Ç–∞—Ç—å–∏ –±—Ä–∞—á–Ω–æ–≥–æ –¥–æ–≥–æ–≤–æ—Ä–∞.pdf",
    "–û–±—Ä–∞–∑—Ü—ã —Ç—Ä—É–¥–æ–≤—ã—Ö –¥–æ–≥–æ–≤–æ—Ä–æ–≤": "–°—Ç–∞—Ç—å–∏_–¥–æ–≥–æ–≤–æ—Ä–∞_–Ω–∞–∏ÃÜ–º–∞_—Ä–∞–±–æ—Ç–Ω–∏–∫–∞.pdf",
    "–î–æ–≥–æ–≤–æ—Ä—ã –Ω–∞–π–º–∞: –æ–±—Ä–∞–∑—Ü—ã –∏ —à–∞–±–ª–æ–Ω—ã": "–°—Ç–∞—Ç—å–∏ –¥–æ–≥–æ–≤–æ—Ä–∞ –Ω–∞–π–º–∞ —Ä–∞–±–æ—Ç–Ω–∏–∫–∞.pdf",
    "–û–±—Ä–∞–∑—Ü—ã –¥–æ–≥–æ–≤–æ—Ä–æ–≤ –ø–µ—Ä–µ–≤–æ–∑–∫–∏": "–°—Ç–∞—Ç—å–∏ –¥–æ–≥–æ–≤–æ—Ä–∞ –ì–ø—Ö.pdf",
    "–î–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤–∞ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–≥–æ –ª–∏—Ü–∞": "–°—Ç–∞—Ç—å–∏ –∑–∞–π–º-—Ä–∞—Å–ø–∏—Å–∫–∞.pdf"
}

def check_contract_by_detected_topic(contract_pdf_path: str, topic: str) -> str:
    kodex_filename = TOPIC_TO_KODEX_PDF.get(topic)
    if not kodex_filename:
        raise ValueError(f"‚ùó –ù–µ—Ç PDF —Å—Ç–∞—Ç–µ–π –¥–ª—è —Ç–µ–º—ã: {topic}")
    
    kodex_path = os.path.join("kodex", kodex_filename)
    if not os.path.exists(kodex_path):
        raise FileNotFoundError(f"‚ùó –ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª: {kodex_path}")

    contract_text = "\n".join(page.get_text() for page in fitz.open(contract_pdf_path))
    gk_text = "\n".join(page.get_text() for page in fitz.open(kodex_path))

    # –î–µ–ª–∏–º –Ω–∞ —Å—Ç–∞—Ç—å–∏
    articles = []
    current = []
    title = None
    for line in gk_text.splitlines():
        if line.strip().startswith("–°—Ç–∞—Ç—å—è"):
            if title and current:
                articles.append((title, "\n".join(current)))
            title = line.strip()
            current = []
        else:
            current.append(line)
    if title and current:
        articles.append((title, "\n".join(current)))

    llm = GigaChat(
        credentials=AUTH,
        verify_ssl_certs=False,
        scope="GIGACHAT_API_CORP",
        model="GigaChat-2-Max",
        profanity_check=False
    )

    prompt = PromptTemplate(
        input_variables=["article", "contract"],
        
        template = '''
    –¢—ã ‚Äî –∫–≤–∞–ª–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —é—Ä–∏—Å—Ç —Å –æ–ø—ã—Ç–æ–º –¥–æ–≥–æ–≤–æ—Ä–Ω–æ–π –∏ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–π –ø—Ä–∞–∫—Ç–∏–∫–∏. 
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–∏ **—Ñ—Ä–∞–≥–º–µ–Ω—Ç –¥–æ–≥–æ–≤–æ—Ä–∞** —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º —É–∫–∞–∑–∞–Ω–Ω–æ–π —Å—Ç–∞—Ç—å–∏ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–∞.

1. –ï—Å–ª–∏ —Å—Ç–∞—Ç—å—è –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ **–ì—Ä–∞–∂–¥–∞–Ω—Å–∫–æ–º—É –∫–æ–¥–µ–∫—Å—É –†–§**, –Ω–æ —Ç–µ–∫—Å—Ç ‚Äî –Ω–∞–ø—Ä–∏–º–µ—Ä, –∏–∑ **—Ç—Ä—É–¥–æ–≤–æ–≥–æ –¥–æ–≥–æ–≤–æ—Ä–∞**, —É–∫–∞–∂–∏, —á—Ç–æ —Å–ª–µ–¥—É–µ—Ç —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ–≤–∞—Ç—å—Å—è –¢—Ä—É–¥–æ–≤—ã–º –∫–æ–¥–µ–∫—Å–æ–º –†–§, –∞ –Ω–µ –ì–ö –†–§.
2. –ï—Å–ª–∏ —Å—Ç–∞—Ç—å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞ –ø–æ –ø—Ä–µ–¥–º–µ—Ç—É, –ø—Ä–æ–≤–µ–¥–∏ **—é—Ä–∏–¥–∏—á–µ—Å–∫–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ**: —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ, –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è, –ø—Ä–æ–±–µ–ª—ã.
3. –ù–∞—Ä—É—à–µ–Ω–∏—è —Ñ–æ—Ä–º—É–ª–∏—Ä—É–π **—á—ë—Ç–∫–æ**, —Å –ø–æ—è—Å–Ω–µ–Ω–∏–µ–º, –ø–æ—á–µ–º—É –æ–Ω–∏ –Ω–∞—Ä—É—à–∞—é—Ç –Ω–æ—Ä–º—É (–Ω–∞–ø—Ä–∏–º–µ—Ä: "–°—Ç–∞—Ç—å—è 689 —Ç—Ä–µ–±—É–µ—Ç –ø–∏—Å—å–º–µ–Ω–Ω–æ–π —Ñ–æ—Ä–º—ã ‚Äî –≤ —Ç–µ–∫—Å—Ç–µ –µ—ë –Ω–µ—Ç").
4. –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä—É–π –æ—Ç–≤–µ—Ç: **"–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ / –ù–∞—Ä—É—à–µ–Ω–∏—è / –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"**.
5. –í –∫–æ–Ω—Ü–µ –¥–∞–π –∫—Ä–∞—Ç–∫–∏–π **—é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –≤—ã–≤–æ–¥**: "–ù–∞—Ä—É—à–µ–Ω–∏–π –Ω–µ –≤—ã—è–≤–ª–µ–Ω–æ" –∏–ª–∏ "–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –Ω–∞—Ä—É—à–µ–Ω–∏—è".

–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:

üìò –°—Ç–∞—Ç—å—è (—Å—Å—ã–ª–∫–∞ –Ω–∞ –Ω–æ—Ä–º—É):
{article}

üìÑ –§—Ä–∞–≥–º–µ–Ω—Ç –¥–æ–≥–æ–≤–æ—Ä–∞:
{contract}

‚öñÔ∏è –û—Ç–≤–µ—Ç:
'''
    )

    chain = LLMChain(llm=llm, prompt=prompt)
    trimmed_contract = truncate(contract_text)

    results = []
    for title, content in articles:
        print(f"\nüìò {title}:\n{content[:300]}...\n{'-'*40}")
        result = chain.invoke({
            "article": truncate(content),
            "contract": trimmed_contract
        })
        results.append(f"üîπ {title}:\n{result['text'].strip()}\n")

    '''out_path = "gk_topic_audit.txt"
    with open(out_path, "w", encoding="utf-8") as f:
        f.write("\n".join(results))'''
    
    full_audit_path = "gk_topic_audit.txt"
    with open(full_audit_path, "w", encoding="utf-8") as f:
        f.write("\n".join(results))

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫—Ä–∞—Ç–∫–æ–µ —Å–∞–º–º–∞—Ä–∏
    summary_prompt = PromptTemplate(
        input_variables=["full_audit"],
        template="""
–¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π —é—Ä–∏—Å—Ç. –£ —Ç–µ–±—è –Ω–∞ —Ä—É–∫–∞—Ö –±–æ–ª—å—à–æ–π –∞–Ω–∞–ª–∏–∑ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –¥–æ–≥–æ–≤–æ—Ä–∞ –Ω–æ—Ä–º–∞–º –ì—Ä–∞–∂–¥–∞–Ω—Å–∫–æ–≥–æ –∫–æ–¥–µ–∫—Å–∞ –†–§. –ù–∞ –æ—Å–Ω–æ–≤–µ –Ω–µ–≥–æ:

1. –°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –∫—Ä–∞—Ç–∫–æ–µ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–µ **—Å–∞–º–º–∞—Ä–∏ –Ω–∞ 1‚Äì2 —Å—Ç—Ä–∞–Ω–∏—Ü—ã** (1500‚Äì2000 —Å–∏–º–≤–æ–ª–æ–≤).
2. –°—Ç—Ä—É–∫—Ç—É—Ä–∞:
- **–û—Å–Ω–æ–≤–Ω—ã–µ –≤—ã—è–≤–ª–µ–Ω–Ω—ã–µ –Ω–∞—Ä—É—à–µ–Ω–∏—è**
- **–†–∏—Å–∫–∏ (–µ—Å–ª–∏ –æ—Å—Ç–∞–≤–∏—Ç—å –∫–∞–∫ –µ—Å—Ç—å)**
- **–ö—Ä–∞—Ç–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏**
- **–Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –≤—ã–≤–æ–¥**
3. –ü–∏—à–∏ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏ —á—ë—Ç–∫–æ, –±–µ–∑ –≤–æ–¥—ã, –±–µ–∑ –ø–æ–≤—Ç–æ—Ä–æ–≤.

üìÑ –ê–Ω–∞–ª–∏–∑:
{full_audit}

‚úÇÔ∏è –ö—Ä–∞—Ç–∫–æ–µ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–µ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ:
"""
    )

    summary_chain = LLMChain(llm=llm, prompt=summary_prompt)
    summary_result = summary_chain.invoke({"full_audit": "\n".join(results)})

    summary_path = "gk_topic_summary.txt"
    with open(summary_path, "w", encoding="utf-8") as f:
        f.write(summary_result["text"].strip())

    # –í–µ—Ä–Ω—É—Ç—å –æ–±–∞ –ø—É—Ç–∏
    return full_audit_path, summary_path